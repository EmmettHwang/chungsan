"""
RAG ì²´ì¸ ëª¨ë“ˆ
ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ì‘ë‹µ ìƒì„±
"""

from typing import List, Dict, Optional
import httpx

# LangChain imports - ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
try:
    from langchain_core.documents import Document
except ImportError:
    from langchain.schema import Document


class RAGChain:
    """RAG ì²´ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, vector_store_manager, api_key: str, api_type: str = "groq"):
        """
        Args:
            vector_store_manager: VectorStoreManager ì¸ìŠ¤í„´ìŠ¤
            api_key: AI API í‚¤ (GROQ, Gemini ë“±)
            api_type: API íƒ€ì… ('groq', 'gemini', 'gemma')
        """
        self.vector_store = vector_store_manager
        self.api_key = api_key
        self.api_type = api_type.lower()
    
    def _format_context(self, documents: List[Document]) -> str:
        """
        ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        
        Args:
            documents: ê²€ìƒ‰ëœ Document ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í¬ë§·ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if not documents:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        context_parts = []
        for i, doc in enumerate(documents, 1):
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            content = doc.page_content.strip()
            
            context_parts.append(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {source}\n{content}")
        
        return "\n\n".join(context_parts)
    
    def _build_prompt(self, query: str, context: str, system_message: Optional[str] = None) -> str:
        """
        RAG í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°œì„ ëœ ë²„ì „)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            system_message: ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ì„ íƒ)
            
        Returns:
            ì™„ì„±ëœ í”„ë¡¬í”„íŠ¸
        """
        if system_message is None:
            system_message = """ë‹¹ì‹ ì€ ë°”ì´ì˜¤í—¬ìŠ¤ ë¶„ì•¼ ì „ë¬¸ êµìœ¡ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."""
        
        prompt = f"""{system_message}

=== ì°¸ê³  ë¬¸ì„œ ===
{context}

=== ì§ˆë¬¸ ===
{query}

=== ë‹µë³€ ê·œì¹™ ===
1. ë¬¸ì„œì— ëª…í™•í•œ ë‹µì´ ìˆìœ¼ë©´ ì •í™•íˆ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ë‹µì´ ì—†ìœ¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. ì¶”ì¸¡í•˜ê±°ë‚˜ ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
4. êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”
5. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš” (ë¶ˆí•„ìš”í•œ ë°˜ë³µ ê¸ˆì§€)

=== ë‹µë³€ ===
"""
        
        return prompt
    
    async def _call_groq_api(self, prompt: str) -> str:
        """GROQ API í˜¸ì¶œ"""
        url = "https://api.groq.com/openai/v1/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "llama-3.3-70b-versatile",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,  # ì •í™•ë„ ìš°ì„ 
            "max_tokens": 8000,  # ë¬¸ì œ ìƒì„± ë“± ê¸´ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
            "top_p": 0.9
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=payload, timeout=120.0)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            response.raise_for_status()
            data = response.json()
            
            if data.get('choices'):
                return data['choices'][0]['message']['content']
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def _call_gemini_api(self, prompt: str) -> str:
        """Gemini API í˜¸ì¶œ"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent?key={self.api_key}"
        
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.3,
                "maxOutputTokens": 1000,
                "topP": 0.9
            }
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, json=payload, timeout=30.0)
            response.raise_for_status()
            data = response.json()
            
            if data.get('candidates'):
                return data['candidates'][0]['content']['parts'][0]['text']
            else:
                return "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def query(self,
                    question: str,
                    k: int = 5,  # 3ì—ì„œ 5ë¡œ ì¦ê°€
                    system_message: Optional[str] = None,
                    min_similarity: float = 0.3,  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì¶”ê°€
                    document_context: Optional[List[str]] = None,  # íŠ¹ì • ë¬¸ì„œë¡œ ì œí•œ
                    groq_api_key: Optional[str] = None) -> Dict:  # API í‚¤ ë™ì  ì „ë‹¬
        """
        RAG ì§ˆë¬¸ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜ (ê¸°ë³¸ê°’ 5ë¡œ ì¦ê°€)
            system_message: ì»¤ìŠ¤í…€ ì‹œìŠ¤í…œ ë©”ì‹œì§€
            min_similarity: ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0~1.0, ê¸°ë³¸ê°’ 0.3)
            document_context: íŠ¹ì • ë¬¸ì„œëª… ëª©ë¡ìœ¼ë¡œ í•„í„°ë§ (ì„ íƒ)
            groq_api_key: GROQ API í‚¤ (ì„ íƒ, ë™ì  ì „ë‹¬)

        Returns:
            {
                'answer': AI ì‘ë‹µ,
                'sources': ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸,
                'context': ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
            }
        """
        # API í‚¤ê°€ ì „ë‹¬ë˜ë©´ ì‚¬ìš©
        if groq_api_key:
            self.api_key = groq_api_key

        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            print(f"[DEBUG] ì§ˆë¬¸: {question[:100]}...")
            print(f"[DOC] {k}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")

            documents = self.vector_store.search_with_score(question, k=k * 2 if document_context else k)

            # ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ê°€ ì§€ì •ëœ ê²½ìš° í•„í„°ë§
            if document_context and len(document_context) > 0:
                print(f"[FILTER] {len(document_context)}ê°œ ë¬¸ì„œë¡œ í•„í„°ë§: {document_context}")
                filtered_docs = []
                for doc in documents:
                    metadata = doc.get('metadata', {})
                    filename = metadata.get('original_filename') or metadata.get('filename', '')
                    # íŒŒì¼ëª…ì´ ì„ íƒëœ ë¬¸ì„œ ëª©ë¡ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if any(ctx in filename or filename in ctx for ctx in document_context):
                        filtered_docs.append(doc)
                documents = filtered_docs[:k]
                print(f"[FILTER] í•„í„°ë§ í›„ {len(documents)}ê°œ ë¬¸ì„œ")
            
            if not documents:
                return {
                    'answer': "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”.",
                    'sources': [],
                    'context': ""
                }
            
            # 2. ìœ ì‚¬ë„ ì²´í¬ - ëª¨ë“  ë¬¸ì„œì˜ ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ê²½ê³ 
            max_similarity = max([doc_dict.get('score', 0) for doc_dict in documents])
            print(f"[INFO] ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.2%}")
            
            if max_similarity < min_similarity:
                return {
                    'answer': f"ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nğŸ’¡ íŒ: ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì§ˆë¬¸í•˜ê±°ë‚˜, ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.\n(ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìµœëŒ€ ìœ ì‚¬ë„: {max_similarity:.1%})",
                    'sources': [],
                    'context': ""
                }
            
            # 3. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (SimpleVectorStore í˜•ì‹)
            context_parts = []
            for i, doc_dict in enumerate(documents, 1):
                metadata = doc_dict.get('metadata', {})
                source = metadata.get('original_filename') or metadata.get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')
                subject = metadata.get('subject', '')
                content = doc_dict.get('content', '').strip()
                similarity = doc_dict.get('score', 0)
                
                source_info = f"{source}"
                if subject:
                    source_info += f" ({subject})"
                
                context_parts.append(f"[ë¬¸ì„œ {i}] ì¶œì²˜: {source_info} (ìœ ì‚¬ë„: {similarity:.1%})\n{content}")
            
            context = "\n\n".join(context_parts)
            
            print(f"[OK] {len(documents)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            
            # 4. í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_prompt(question, context, system_message)
            
            # 5. AI API í˜¸ì¶œ
            print(f"[AI] {self.api_type.upper()} API í˜¸ì¶œ ì¤‘...")
            
            if self.api_type == 'groq' or self.api_type == 'gemma':
                answer = await self._call_groq_api(prompt)
            elif self.api_type == 'gemini':
                answer = await self._call_gemini_api(prompt)
            else:
                answer = "ì§€ì›í•˜ì§€ ì•ŠëŠ” API íƒ€ì…ì…ë‹ˆë‹¤."
            
            print(f"[OK] ì‘ë‹µ ìƒì„± ì™„ë£Œ")
            
            # 6. ì¶œì²˜ ì •ë³´ ì¶”ì¶œ (SimpleVectorStore í˜•ì‹)
            sources = []
            for doc_dict in documents:
                metadata = doc_dict.get('metadata', {})
                source_name = metadata.get('original_filename') or metadata.get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')
                subject = metadata.get('subject', '')
                
                source_display = source_name
                if subject:
                    source_display = f"{source_name} - {subject}"
                
                sources.append({
                    'source': source_display,
                    'content': doc_dict.get('content', '')[:200] + '...',
                    'similarity': float(doc_dict.get('score', 0)),  # 0~1 ë²”ìœ„ë¡œ ë°˜í™˜
                    'metadata': metadata
                })
            
            return {
                'answer': answer,
                'sources': sources,
                'context': context
            }
            
        except Exception as e:
            print(f"[ERROR] RAG ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {
                'answer': f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                'sources': [],
                'context': ""
            }
    
    async def query_simple(self, question: str, k: int = 3) -> str:
        """
        ê°„ë‹¨í•œ RAG ì§ˆë¬¸ (ë‹µë³€ë§Œ ë°˜í™˜)
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            
        Returns:
            AI ì‘ë‹µ ë¬¸ìì—´
        """
        result = await self.query(question, k=k)
        return result['answer']


if __name__ == "__main__":
    import asyncio
    import os
    from document_loader import DocumentLoader
    from vector_store import VectorStoreManager
    
    async def test_rag():
        # ë¬¸ì„œ ë¡œë”
        loader = DocumentLoader(chunk_size=500, chunk_overlap=50)
        
        # ìƒ˜í”Œ ë¬¸ì„œ
        sample_text = """
        ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—… ê°œìš”
        
        ë°”ì´ì˜¤í—¬ìŠ¤ ì‚°ì—…ì€ ìƒëª…ê³µí•™ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì¸ê°„ì˜ ê±´ê°•ê³¼ ì‚¶ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ì‚°ì—…ì…ë‹ˆë‹¤.
        ì£¼ìš” ë¶„ì•¼ë¡œëŠ” ì‹ ì•½ ê°œë°œ, ì˜ë£Œê¸°ê¸°, ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ë“±ì´ ìˆìŠµë‹ˆë‹¤.
        
        mRNA ë°±ì‹  ê¸°ìˆ 
        
        mRNA ë°±ì‹ ì€ ë©”ì‹ ì € RNAë¥¼ ì´ìš©í•˜ì—¬ ìš°ë¦¬ ëª¸ì˜ ì„¸í¬ê°€ íŠ¹ì • ë‹¨ë°±ì§ˆì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
        ì´ ê¸°ìˆ ì€ COVID-19 íŒ¬ë°ë¯¹ ë™ì•ˆ ë¹ ë¥´ê²Œ ë°œì „í•˜ì˜€ìœ¼ë©°, í–¥í›„ ì•” ì¹˜ë£Œ ë“±ì—ë„ í™œìš©ë  ì „ë§ì…ë‹ˆë‹¤.
        """
        
        os.makedirs("./test_docs", exist_ok=True)
        with open("./test_docs/sample.txt", "w", encoding="utf-8") as f:
            f.write(sample_text)
        
        # ë¬¸ì„œ ë¡œë“œ
        docs = loader.load_document("./test_docs/sample.txt", {"subject": "ë°”ì´ì˜¤í—¬ìŠ¤ ê¸°ì´ˆ"})
        
        # ë²¡í„° ìŠ¤í† ì–´
        vector_store = VectorStoreManager(
            persist_directory="./test_chroma_db",
            collection_name="test_collection"
        )
        vector_store.add_documents(docs)
        
        # RAG ì²´ì¸ (API í‚¤ í•„ìš”)
        api_key = os.getenv("GROQ_API_KEY", "your-api-key-here")
        rag_chain = RAGChain(vector_store, api_key, api_type="groq")
        
        # ì§ˆë¬¸
        question = "mRNA ë°±ì‹ ì´ ë¬´ì—‡ì¸ê°€ìš”?"
        print(f"\n[Q] ì§ˆë¬¸: {question}\n")
        
        result = await rag_chain.query(question, k=2)
        
        print(f"\n[AI] ë‹µë³€:\n{result['answer']}\n")
        print(f"\n[DOC] ì°¸ê³  ë¬¸ì„œ:")
        for i, source in enumerate(result['sources'], 1):
            print(f"\n  {i}. {source['source']} (ìœ ì‚¬ë„: {source['similarity']:.4f})")
            print(f"     {source['content']}")
    
    # ì‹¤í–‰
    asyncio.run(test_rag())
